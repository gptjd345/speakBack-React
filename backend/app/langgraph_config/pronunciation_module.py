from .store import global_store
import re
from typing import Optional, Dict

from vosk import Model, KaldiRecognizer
import wave, json, os

from pydub import AudioSegment, silence
import io
import subprocess

from openai import OpenAI

def get_client():
    return OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ----------------------------------------------------
# Call AI 
# ----------------------------------------------------
def call_ai(system_prompt: str, user_prompt: str) -> str:
    """
    GPT ê¸°ë°˜ í‰ê°€ í˜¸ì¶œ
    """
    client = get_client()
    response = client.chat.completions.create(
        model="gpt-4o-mini",   # ë¹ ë¥¸ í”¼ë“œë°±ìš©
        # model="gpt-4o",      # ë” ì •ë°€í•˜ê²Œ í•˜ê³  ì‹¶ì„ ë•Œ
        messages=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.2,  # ì¼ê´€ëœ ì ìˆ˜ ìœ ì§€
        response_format={"type": "json_object"}  # JSON ì‘ë‹µ ê°•ì œ
    )
    return response.choices[0].message.content

# ----------------------------------------------------
# Audio data preprocessing (BytesIO â†’ 16kHz mono wav)
# ----------------------------------------------------
def prepare_audio_for_vosk(org_file_path) -> io.BytesIO:
    """
    Streamlit BytesIO / UploadedFile â†’ Voskì—ì„œ ì“¸ ìˆ˜ ìˆëŠ” 16kHz mono wavë¡œ ë³€í™˜
    """
    process = subprocess.run(
        [
            "ffmpeg",
            "-i", org_file_path,
            "-ar", "16000",    # 16kHz
            "-ac", "1",        # mono
            "-f", "wav",
            "pipe:1"
        ],
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        check=True
    )
    return io.BytesIO(process.stdout)

# -----------------------------
# Vosk ëª¨ë¸ ë¡œë“œ
# -----------------------------
VOSK_MODEL_PATH = "vosk-model-small-en-us-0.15"  # ëª¨ë¸ ë‹¤ìš´ë¡œë“œ í›„ ê²½ë¡œ
if not os.path.exists(VOSK_MODEL_PATH):
    raise FileNotFoundError("Vosk ëª¨ë¸ì„ ë¨¼ì € ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”!")
vosk_model = Model(VOSK_MODEL_PATH)

def stt_vosk(user_audio_path: str) :
    """ì‚¬ìš©ì ìŒì„±íŒŒì¼ì„ Vosk STTë¡œ ë³€í™˜"""
    audio_stream = prepare_audio_for_vosk(user_audio_path)

    # Vosk recognizer ì´ˆê¸°í™”
    model = Model(VOSK_MODEL_PATH)
    rec = KaldiRecognizer(model, 16000)
    rec.SetWords(True)

    # wav header skip í•„ìš” â†’ wave ëª¨ë“ˆ ì‚¬ìš© X, raw bytes ê·¸ëŒ€ë¡œ ì²˜ë¦¬
    while True:
        data = audio_stream.read(4000)
        if len(data) == 0:
            break
        rec.AcceptWaveform(data)

    result = json.loads(rec.FinalResult())
    text = result.get("text", "").strip()
    words = result.get("result", [])  # ë‹¨ì–´ë³„ confidence

    conf_dict = {w["word"]: w.get("conf", 0) for w in words}
    return text, conf_dict

# Try import Coqui TTS (optional)
try:
    from TTS.api import TTS
    # ì•± ì‹œì‘ ì‹œ í•œ ë²ˆë§Œ ë¡œë“œ
    # ëª¨ë¸ ì„ íƒ: (ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ì´ë¦„ì€ í™˜ê²½ì— ë”°ë¼ ë°”ê¿”ì•¼ í•¨)
    # ì˜ˆ: LJSpeech â†’ ë¯¸êµ­ ì—¬ì„± í™”ì ë°ì´í„°ì…‹ ê¸°ë°˜
    # ë°œìŒì€ ì „í˜•ì ì¸ American English
    tts_us_model = TTS(model_name="tts_models/en/ljspeech/tacotron2-DDC", 
                    progress_bar=False, gpu=False)
    TTS_AVAILABLE = True
except Exception:
    TTS_AVAILABLE = False
#tts_uk_model = TTS(model_name="tts_models/en/vctk/vits", progress_bar=False, gpu=False)

# -----------------------------
# Audio trimming helper
# -----------------------------
def trim_audio(audio_bytes: bytes, silence_thresh=-40, min_silence_len=200) -> bytes:
    """
    ì˜¤ë””ì˜¤ ëë¶€ë¶„ì˜ ë¶ˆí•„ìš”í•œ ë¬´ìŒ/ì´ìƒìŒì„ ì œê±°
    - silence_thresh: ë¬´ìŒìœ¼ë¡œ ì¸ì‹í•  dB ê¸°ì¤€ (-40~ -50 ì¶”ì²œ)
    - min_silence_len: ì œê±°í•  ìµœì†Œ ë¬´ìŒ ê¸¸ì´(ms)
    """
    audio = AudioSegment.from_file(io.BytesIO(audio_bytes), format="wav")
    
    silences = silence.detect_silence(
        audio,
        silence_thresh=silence_thresh,
        min_silence_len=min_silence_len
    )

    # ëë¶€ë¶„ ë¬´ìŒ ì œê±°
    if silences:
        last_silence = silences[-1]
        if last_silence[1] >= len(audio) - 100:  # ëì— ë¬´ìŒì´ ìˆëŠ” ê²½ìš°
            audio = audio[:last_silence[0]]

    buf = io.BytesIO()
    audio.export(buf, format="wav")
    buf.seek(0)
    return buf.getvalue()

# -----------------------------
# TTS ìƒì„± (ê¸¸ì´ í¬í•¨)
# -----------------------------
def tts_generate_us(text: str) -> tuple[bytes, float]:
    """US tutor TTS â†’ wav ë°”ì´íŠ¸ ë¦¬í„´"""
    wav_path = "reference_us.wav"
    tts_us_model.tts_to_file(text=text, file_path=wav_path)
    with open(wav_path, "rb") as f:
        wav_bytes = f.read()

    # trimming ì ìš©
    trimmed_bytes = trim_audio(wav_bytes)

    seg = AudioSegment.from_file(io.BytesIO(trimmed_bytes), format="wav")
    duration_sec = len(seg) / 1000.0

    return trimmed_bytes, duration_sec
    
# -----------------------------
# Audio duration helper
# -----------------------------
def get_audio_duration(file_path: str) -> float:
    """wav íŒŒì¼ ê¸¸ì´(ì´ˆ)"""
    with wave.open(file_path, "rb") as wf:
        frames = wf.getnframes()
        rate = wf.getframerate()
        return frames / float(rate)   

def evaluate_pronunciation(target_text: str, user_audio_path: str, tutor_type: str = "us"):
    """
    ì „ì²´ íë¦„: ì‚¬ìš©ì ì˜¤ë””ì˜¤ â†’ STT â†’ ë°œìŒ í‰ê°€ â†’ ê²°ê³¼ ë°˜í™˜
    """
    # 1) íŠœí„° ì°¸ì¡° ìŒì„±, íŠœí„° ìŒì„±ì‹œê°„
    ref_audio,ref_duration = tts_generate_us(target_text) if tutor_type == "us" else None

    # 2) ì‚¬ìš©ì ìŒì„± â†’ STT
    user_transcript, conf_dict = stt_vosk(user_audio_path)
    user_seg = AudioSegment.from_file(user_audio_path)
    # ì‚¬ìš©ìë°œí™”ì‹œê°„
    user_duration = len(user_seg) / 1000.0

    # 3) Invoke AI Prompt
    system_prompt ="""You are an English pronunciation tutor. 
    Your job is to evaluate how well a learner pronounced a target phrase.

    Criteria:
    1. Content words (nouns, verbs, adjectives, adverbs) are most important. 
       - Strong weight if clear and confident.
       - Medium if slightly weak but understandable.
       - Low if unclear or missing.
    2. Function words (articles, prepositions, auxiliaries) are less important. 
       - If the learner used natural contractions (I'm, gonna, don't), give extra credit.
       - If omitted but sentence is still natural, small penalty only.
    3. Speed & rhythm: If userâ€™s speech duration is close to native reference duration, give a bonus.
    4. Give short, encouraging feedback for each important word.
    5. Return a total score as a percentage (0-100).
    
    IMPORTANT: Return your final result in JSON format only.
    IMPORTANT SCORING RULES:
        - Always return a score between 50 and 100.
        - If the learnerâ€™s speech is generally understandable, the minimum score should be 70.
        - Only if the speech is completely unintelligible, give below 50.

    example response:
    {
      "score": 87.5,
      "feedback": [
        "Content word 'goal' was clear ğŸ‘",
        "Function word 'to' was slightly weak but acceptable.",
        "Used contraction 'I'm' naturally ğŸ‘Œ"
      ],
      "user_transcript": "...",
      "target_text": "..."
    }
 

    """

    user_prompt = f"""
    Target phrase: "{target_text}"
    Learner transcript: "{user_transcript}"
    Learner duration: {user_duration:.2f} seconds
    Reference duration: {ref_duration:.2f} seconds
    """

    # pseudo code: AI í˜¸ì¶œ
    ai_response = call_ai(system_prompt, user_prompt)

    try :
        result = json.loads(ai_response)
    except json.JSONDecodeError:
        result = {
                "score": 0, 
                "feedback": ["AI ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨"], 
                "user_transcript": user_transcript, 
                "target_text": target_text
            }


    # ìµœì¢… ê²°ê³¼
    total_result = {
        "score": result.get("score", 0),
        "feedback": result.get("feedback", []),
        # "target_chunks": target_chunks,
        "reference_tts": ref_audio,   # US tutor ìŒì„± (wav ë°”ì´íŠ¸)
        "user_transcript": user_transcript,
        "user_duration": user_duration,
        "ref_duration": ref_duration
    }
    return total_result

